{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session\n",
    "\n",
    "## Computational Semantics and Pragmatics, 02/11/2017\n",
    "\n",
    "This is the introductory practical session of the course Computational Semantics and Pragmatics. We will get you started with the corpus that we will use throughout the course and give you a brief introduction to *Jupyter Notebooks*, which we will use for the practical assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the materials\n",
    "\n",
    "Start by downloading the folder *cosp2017_practical* from here: https://surfdrive.surf.nl/files/index.php/s/8MVVoz5sna0LhTg (you may need to expand it). \n",
    "\n",
    "This folder contains the corpus, some Python scripts, and other auxiliary materials that will be used in the different assignments throughout the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Switchboard corpus\n",
    "\n",
    "In the assignmens of this course we will explore a dataset called the Switchboard corpus, consisting of telephone conversations between speakers of American English (https://catalog.ldc.upenn.edu/LDC97S62). There are several different distributions of the Switchboard corpus available (see http://groups.inf.ed.ac.uk/switchboard/structure.html for a description of the project). The distribution of the corpus that we are using is a version we have preprocessed for use in this course, which includes both linguistic annotations (the dialogue acts of the utterances and parsed and POS-annotated versions of these utterances), as well as timing information (beginning and end of turns). This corresponds to a subset of the overall corpus, containing 642 dialogues. You will find this version of the corpus in the folder *swda_time*.\n",
    "\n",
    "The folder contains a bunch of csv files called swXXXX.csv. Each of them correponds to a conversation, with one utterance per row annotated with different types of information. The user manual here https://web.stanford.edu/~jurafsky/ws97/manual.august1.html can be used as a reference to understand the meaning of the different annotation tags.\n",
    "\n",
    "Explore a couple of conversation files by opening them with a csv editor/viewer. **Make sure NOT to save them to avoid that your default csv editor makes changes in the encoding.** \n",
    "\n",
    "The folder also contains a file *swda-metadata-ext.csv* with metadata for all conversations in the corpus (and also for the conversations that are not in this specific distribution), such as the identities of the speakers and the topic of the conversation. Take a look at it, and again don't modify it nor save it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook\n",
    "\n",
    "For the assignments we will use an environment called _Jupyter Notebook_ (formerly known as iPython notebook), in which code and Markdown text can be combined (for more information, see http://jupyter.readthedocs.io/en/latest/index.html). You can view the instructions in an online notebook viewer, but to edit code and hand in your assignments, you will need to install Jupyter Notebook on your own machine. The Notebook environment is most easily installed using *Anaconda*, but you can also use *pip*. How exactly you should install it depends on your operating system, for instructions check https://jupyter.readthedocs.io/en/latest/install.html.\n",
    "\n",
    "**Important: the code we will use is written in Python 2 and is not upwards compatible. In particular, if you install Anaconda, make sure you choose the Python 2.7 version. Adding a Python 2 kernel to an existing Jupyter installation might also work, but is not recommended.**\n",
    "\n",
    "Now install iPython/Jupyter Notebook on the machine you are using, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Python scripts and libraries\n",
    "\n",
    "To extract (meta)information from the corpus, we will use a set of Python classes provided by Chris Potts (http://compprag.christopherpotts.net/swda.html). The distribution of the corpus that the scripts assume is slightly different from ours, so we will use an adapted version of these scripts *swda_time.py*, that you can find in the main folder *cosp2017_practical*.\n",
    "\n",
    "These scripts depend on the latest version of a library called nltk, so make sure you have that installed (http://www.nltk.org/install.html). If you already have nltk installed, you can check your current version in the terminal with the following command:\n",
    "\n",
    "$ python -c \"import nltk; print(nltk.\\__version\\__)\"\n",
    "\n",
    "If you use pip, you can upgrade using the flag -U:\n",
    "\n",
    "$ pip install -U nltk\n",
    "\n",
    "Anaconda allows you to specify the version when you install nltk, choose the latest version 3.2.1.\n",
    "\n",
    "If you are working on a computer where you cannot install packages or you have some other code depending on older versions of nltk, consider creating a virtual environment using virtualenv to create the proper setup\n",
    "\n",
    "To be able to run the code in the notebooks without problems, it is important that the notebook has access to the python classes we are using. Download the ipynb notebook file *practical_session.ipynb* from https://surfdrive.surf.nl/files/index.php/s/FS3EkC09xUuqFlD and copy it into your *cosp2017_practical* folder. \n",
    "\n",
    "\n",
    "## Using the notebook\n",
    "\n",
    "Now you are ready to start up your notebook. Fire up a terminal, navigate to the folder *cosp2017_practical* containing your files and run \"jupyter notebook\" (if you already had ipython notebook installed, you may also use that). You should now see the contents of your folder on the web browser, and you should be able to open and edit the notebook files in it.\n",
    "\n",
    "Some things that may be helpful when working with Jupyter notebooks:\n",
    "\n",
    "* The notebook can be seen as an interactive python session, which means that every time you open your notebook you have to rerun the blocks of code you need. If your code in block X depends on an import in block Y, or uses a variable created in block Y, you have to run block Y before you can run block X. You can run all cells in the notebook by using the menu: *Cell > Run All*.\n",
    "* The shortcuts for running a cell when your cursor is in it are *Shift-Enter* (run cell and go to the next one) and *Ctrl-Enter* (run cell in place). Find more shortcuts in the Help menu.\n",
    "* If your notebook crashes somehow or seems to be unresponsive, consider using the menu to restart/interrupt the kernel in the Kernel menu. If you restart the kernel, your variables will be lost.\n",
    "\n",
    "Now open your own version of this notebook, so that you can play around with the corpus and test if everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Switchboard corpus\n",
    "\n",
    "We will now do a small exploration of the Switchboard corpus and the scripts that can be used to extract information from it. Let's start with looking at a single conversation.\n",
    "\n",
    "### Transcripts\n",
    "\n",
    "The file *swda_time.py* contains a _Transcript_ class. You can create a transcript object of one of the csv files by calling it on this file and the general metadata file that is also in the corpus folder, E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from swda_time import Transcript\n",
    "trans = Transcript('swda_time/sw2020.csv', 'swda_time/swda-metadata-ext.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcript objects have several accessible attributes, such as the date and topic of the dialogue, the age, gender and dialect of the two speakers and the transcription of the utterances spoken in the dialogue (for a complete list, check section 2.2.1 in  http://compprag.christopherpotts.net/swda.html, or look in the metadata file). Since these are phone conversations, the speakers are refered to as 'callers' (each dialogue is a call 'from A to B'). All attributes are accessible just by their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of speaker 'A': 1176\n",
      "Id of speaker 'B': 1169\n",
      "Dialect area of B: NORTH MIDLAND\n",
      "Gender of B: MALE\n",
      "Education of B: 2\n",
      "Topic description of conversation: MUSIC\n",
      "Number of utterances in conversation: 269\n"
     ]
    }
   ],
   "source": [
    "print \"Id of speaker 'A': %s\" % trans.from_caller_id\n",
    "print \"Id of speaker 'B': %s\" % trans.to_caller_id\n",
    "print \"Dialect area of B: %s\" % trans.to_caller_dialect_area\n",
    "print \"Gender of B: %s\" % trans.to_caller_sex\n",
    "print \"Education of B: %s\" % trans.to_caller_education\n",
    "print \"Topic description of conversation: %s\" % trans.topic_description\n",
    "print \"Number of utterances in conversation: %i\" % len(trans.utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utterances\n",
    "\n",
    "The individual utterances, that correspond with rows in the csv file, are *Utterance* objects, that also carry several types of metadata, e.g., the sex, birth year, education level and dialect area of the speaker. Furhermore, the utterance object contains the transcription of the utterance, a POS tagged version and a parse tree, the dialogue act performed by the utterance, the number of the turn the utterance belongs to, and the start and end time of this turn (for a complete list check the Utterance class in the source code). You can access all the attributes in a way similar to the Transcript class, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance 1:\tHi. /\t Speaker: A \t dialogue act: fp\t start turn: 1.747375\t end turn: 2.100750\n",
      "Utterance 2:\tHi,  /\t Speaker: B \t dialogue act: fp^m \t start turn: 2.539000\t end turn: 9.651625\n"
     ]
    }
   ],
   "source": [
    "utt1, utt2 = trans.utterances[0:2]\n",
    "\n",
    "print \"Utterance 1:\\t%s\\t Speaker: %s \\t dialogue act: %s\\t start turn: %f\\t end turn: %f\" \\\n",
    "        % (utt1.text, utt1.caller, utt1.act_tag, utt1.start_turn, utt1.end_turn)\n",
    "print \"Utterance 2:\\t%s\\t Speaker: %s \\t dialogue act: %s \\t start turn: %f\\t end turn: %f\" \\\n",
    "        % (utt2.text, utt2.caller, utt2.act_tag, utt2.start_turn, utt2.end_turn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to look at the meanings of the dialogue act tags here:  http://compprag.christopherpotts.net/swda.html#tags, we will use them in other assignments. \n",
    "\n",
    "Sometimes, a turn can consist of multiple utterances, in which case the turn_index stays consistent over consecutive utterances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {C But, }  the, {F uh, } - /\n",
      "2 there's such a wide selection,  /\n",
      "3 [ I think I like a lot, + I like a little bit of a lot of ] different types of music.  /\n",
      "4 {D You know, } [ [ I, + I, ] + I ] like music  [ that  is, +  that I feel ] - /\n",
      "5 if it is performed correctly or if it's done right, or if the version is done right, I like it <laughter>, /\n",
      "1 Yeah. /\n",
      "\n",
      "speaker: B\tturn index: 14\t subutterance index: 1\n",
      "speaker: B\tturn index: 14\t subutterance index: 2\n",
      "speaker: B\tturn index: 14\t subutterance index: 3\n",
      "speaker: B\tturn index: 14\t subutterance index: 4\n",
      "speaker: B\tturn index: 14\t subutterance index: 5\n",
      "speaker: A\tturn index: 15\t subutterance index: 1\n"
     ]
    }
   ],
   "source": [
    "utterances = trans.utterances[25:31]\n",
    "\n",
    "for utterance in utterances:\n",
    "    print utterance.subutterance_index, utterance.text\n",
    "\n",
    "print \"\\n\",\n",
    "    \n",
    "for utterance in utterances:\n",
    "    print \"speaker: %s\\tturn index: %i\\t subutterance index: %s\" \\\n",
    "        % (utterance.caller, utterance.turn_index, utterance.subutterance_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most utterances also contain a list of syntactic parse trees, that are represented as nltk Trees objects (http://www.nltk.org/_modules/nltk/tree.html) in our script. This means that you can use the methods of this class to access a tree node, for instance the root node of the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utterance text: {D Well, } I mostly listen to popular music.  /\n",
      "\n",
      "corresponding tree: (S\n",
      "  (INTJ (UH Well))\n",
      "  (, ,)\n",
      "  (NP-SBJ (PRP I))\n",
      "  (ADVP (RB mostly))\n",
      "  (VP (VBP listen) (PP (IN to) (NP (JJ popular) (NN music))))\n",
      "  (. .)\n",
      "  (-DFL- E_S))\n",
      "\n",
      "rootnote label: S\n"
     ]
    }
   ],
   "source": [
    "utterance = trans.utterances[4]\n",
    "tree = utterance.trees[0]\n",
    "root_label = tree.label()\n",
    "print \"Utterance text: %s\\n\\ncorresponding tree: %s\\n\\nrootnote label: %s\" % (utterance.text, tree, root_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Utterance class also contains several additional methods, such as the *damsl_act_tag()* method, which collapses the more than 200 dialogue act tags into a reduced set of 44 tags (see http://compprag.christopherpotts.net/swda.html#tags). And also the *pos_lemmas()* method, which  returns a list of the words in the utterance (without the extra annotations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CorpusReader\n",
    "\n",
    "The *swda_time.py* file also contains a class that allows you to work on all dialogues in the corpus directly. This class, called _CorpusReader_, has a method to iterate over all transcripts in the corpus (*iter_transcripts*) or all utterances in all transcripts in the corpus (*iter_utterances*). Using this class, you can easily extract information about the entire corpus, for instance, a distribution over the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "transcript 645"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many dialogues are about the weather: 11 \n",
      "How many dialogues are about music: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from swda_time import CorpusReader\n",
    "from collections import defaultdict\n",
    "\n",
    "corpus = CorpusReader('swda_time', 'swda_time/swda-metadata-ext.csv')\n",
    "\n",
    "topics = defaultdict(int)\n",
    "#dialogue\n",
    "for transcript in corpus.iter_transcripts(display_progress=True):\n",
    "    topics[transcript.topic_description] += 1\n",
    "\n",
    "print \"How many dialogues are about the weather:\", topics['WEATHER CLIMATE'],\n",
    "print \"\"\n",
    "print \"How many dialogues are about music:\", topics['MUSIC'],\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the average number of utterances per turn and what percentage of the utterances are questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of utterances per turn:  1.78885387295\n",
      "Maximum number of utterances per turn:  30\n",
      "\n",
      "Percentage of utterances that are questions: 4.466641%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "utterances_per_turn = []\n",
    "turn_index = 1\n",
    "no_utterances = 0\n",
    "questions = 0\n",
    "#for utterance in corpus.iter_utterances(display_progress=True):\n",
    "for utterance in corpus.iter_utterances(display_progress=False):\n",
    "    if utterance.damsl_act_tag()[0] == 'q':\n",
    "        questions+=1\n",
    "        \n",
    "    if utterance.turn_index == turn_index:\n",
    "        no_utterances+=1\n",
    "    else:\n",
    "        utterances_per_turn.append(no_utterances)\n",
    "        turn_index = utterance.turn_index\n",
    "        no_utterances=1\n",
    "\n",
    "print \"Average number of utterances per turn: \", np.mean(utterances_per_turn)\n",
    "print \"Maximum number of utterances per turn: \", np.max(utterances_per_turn)\n",
    "\n",
    "print \"\\nPercentage of utterances that are questions: %f%%\" % (float(questions)/np.sum(utterances_per_turn)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with the first assignment\n",
    "\n",
    "We have now seen a few examples of how information can be extracted from the Switchboard corpus. When you finish going through the above examples, you can try to extract data from the corpus that you think is interesting, or you can start with the first assignment.\n",
    "\n",
    "Download the first assignment from https://surfdrive.surf.nl/files/index.php/s/4whnfKWM6JJYFhJ. Save the file in your *cosp2017_practical* folder and follow the instructions above to get it running. Try to at least open the assignment and run the cells in it during this practical session, so that you can ask for help if you run into trouble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv2",
   "language": "python",
   "name": "tfenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
